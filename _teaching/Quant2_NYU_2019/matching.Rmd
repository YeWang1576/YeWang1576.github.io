---
title: "Quant II"
subtitle: "Matching and Weighting"
author: "Ye Wang"
date: "2/28/2018"
output: beamer_presentation

---


# Outline
- Last homework  
- Matching   
    - Why matching?
    - Various algorithms
    - Asymptotics of matching
- IPW   
    - Why do we love/hate it?
    - CBPS and PW

# Why matching?

- To get rid of model dependence   
Matching is completely nonparametric: $\hat{\tau}_{i} = Y_i - \sum_{\mathcal{M_i}}Y_{i \in \mathcal{M_i}}$.
\pause
- To estimate heterogeneous treatment effects  
Straightforward.
\pause
- To guarantee common support (positivity)  
Suppose we estimate $\tau$ using Lin's approach, then,
$$\hat{\tau} = \bar{Y}_1 - \bar{Y}_0 - (\frac{N_0}{N_0 + N_1} * \hat{\beta_1} + \frac{N_1}{N_0 + N_1} * \hat{\beta_0})'(\bar{X}_1 - \bar{X}_0)$$
(Imbens and Wooldridge, 2009)   
Bias disappears only when $\bar{X}_1 = \bar{X}_0$ (LaLonde, 1986). 
\pause
- Matching cannnot help you get unconfoundedness.

# Basic steps
1. Choose a distance metric
\pause
2. Find matches on your set of covariates/propensity scores, and get rid of non-matches \pause (Warning!)
\pause
3. Check balance in your matched data set
\pause
4. Repeat these steps until your set exhibits acceptable balance
\pause
5. Calulate the ATT/ATE on your matched dataset 

# An example
- Boyd et al. (2010)
- The effect of gender on decision making
- Unit of analysis: the appellate court case
- Treatment: whether there is at least one female in the three judge panel
- Covariates: median ideology, median age, one racial minority, indicator for ideological direction of lower court's decision, indicator for whether a majority of the judges were nominated by Republicans, indicator for whether a majority of the judges on the panel had judicial experience prior to their nomination

```{r data-loading, echo=FALSE}
# Set up data
d <- read.csv("title7_race.csv")
d$LiberalOutcome <- as.integer(d$case_outcome == 'Liberal')
d$Female <- as.integer(d$gender_judge == 'Female')
d$LiberalLowerDirection <- as.integer(d$lower_dir == 'Liberal')
d$Republican <- as.integer(d$party_judge == 'Republican')
d$Experienced <- as.integer(d$jud_experience == 'Experienced')
d$Minority <- as.integer(d$race_judge != 'White')
d$jcs <- d$jcs - min(d$jcs)
d$confirm_yr <- d$confirm_yr - min(d$confirm_yr)
d$Age <- d$dec_year - d$year_birth

case.order <- unique(d$order) # collapse covariates on cases
median.ideo <- unlist(lapply(case.order, function(x) median(d[d$order == x,]$jcs)))
repub.majority <- as.integer(unlist(lapply(case.order, function(x) sum(d[d$order == x,]$Republican) > 1)))
has.minority <- as.integer(unlist(lapply(case.order, function(x) sum(d[d$order == x,]$Minority) > 0)))
maj.experienced <- as.integer(unlist(lapply(case.order, function(x) sum(d[d$order == x,]$Experienced) > 1)))
has.woman <- as.integer(unlist(lapply(case.order, function(x) sum(d[d$order ==x,]$gender_judge == 'Female') > 0)))
liberal.lower.direction <- unlist(lapply(case.order, function(x) unique(d[d$order == x,]$LiberalLowerDirection)))
median.age <- unlist(lapply(case.order, function(x) median(d[d$order == x,]$Age)))
median.confirmation.year <- unlist(lapply(case.order, function(x) median(d[d$order == x,]$confirm_yr)))
ideo.range <- unlist(lapply(case.order, function(x) diff(range(d[d$order == x,]$jcs))))
minorityXmedianIdeo <- median.ideo * has.minority
liberalOutcome <- unlist(lapply(case.order, function(x) unique(d[d$order == x,]$LiberalOutcome)))

# 6 covariates in total
d.new <- data.frame(
  case.order = case.order,
  median.ideo = median.ideo,
  repub.majority = repub.majority,
  has.minority = has.minority,
  maj.experienced = maj.experienced,
  median.age = median.age,
  liberal.lower.direction = liberal.lower.direction,
  has.woman = has.woman,
  liberalOutcome = liberalOutcome)

trt <- d.new$has.woman == 1

means <- apply(d.new[,-8], 2, function(x) tapply(x, trt, mean))
sds <- apply(d.new[-8], 2, function(x) tapply(x, trt, sd))
t.p <- apply(d.new[, -8], 2, function(x) t.test(x[trt], x[!trt])$p.value)
suppressWarnings(ks.p <- apply(d.new[, -8], 2, function(x) ks.test(x[trt], x[!trt])$p.value))
rownames(means) <- rownames(sds) <- c("Treated", "Control")
```

# View Initial Balance
\footnotesize
```{r}
initial.balance <- round(t(rbind(means,t.p,ks.p)),digits=3)[c(2:8),]
initial.balance
```

# Propensity score matching
- Pros: reduce the number of dimensions
- Cons: may not use information in the most efficient way   
\footnotesize
```{r p-score, echo=FALSE}
p.model <- glm(trt ~ median.ideo + median.age + 
                 repub.majority + has.minority +
                 maj.experienced + 
                 liberal.lower.direction,
               d.new, family = "binomial")

pscore.logit <-  predict(p.model, type = "response")
hist(pscore.logit)
```

# Propensity score matching
```{r ps-matching, echo=FALSE}
d.ctl <- subset(d.new, has.woman == 0)
pscore.logit.ctl <- pscore.logit[!trt]
pscore.logit.trt <- pscore.logit[trt]

d.trt <- subset(d.new, has.woman == 1)
matches <- sapply(pscore.logit.trt, function(x) which.min(abs(pscore.logit.ctl-x)))
d.trt <- rbind(d.trt, d.ctl[matches, ])
pm.logit.mod <- lm(liberalOutcome ~ has.woman + median.ideo + median.age + repub.majority + has.minority +maj.experienced + 
                     liberal.lower.direction, d.trt)
N <- length(matches)
plot(c(pscore.logit.trt, pscore.logit.ctl[matches]), jitter(rep(c(1, 0), c(N,N))), axes = F, ylab= "Treatment and Control", 
     xlab = "Propensity Score")
axis(1)
```

# Nearest Neighbor Matching
- Approximate a blocking experiment
- You can also use `MatchIt` 
\footnotesize
```{r NN-formula, echo=FALSE}
library(MatchIt)
matching.formula <- as.formula('has.woman ~ 
    median.ideo + median.age + repub.majority +
     has.minority + maj.experienced + liberal.lower.direction')
```

# Nearest Neighbor Matching
\footnotesize
```{r NN-matching, echo=FALSE}
matched.NN <-matchit(matching.formula, method="nearest", data = d.new)
d.NN <- match.data(matched.NN)
t.NN <- apply(d.NN[, c(2:7)], 2, function(x) t.test(x[d.NN$has.woman==1], x[d.NN$has.woman==0])$p.value)
result.NN <- summary(matched.NN)[3][[1]][-1,]
result.NN <- data.frame(cbind(result.NN, t.NN))
result.NN[,c(1:2,8)]
```



# Nearest Neighbor Matching
\footnotesize
```{r NN-plot}
plot(matched.NN, type="hist", col = "black", breaks = 20)
```

# Genetic Matching
- Set an objective function and update the distance metric iteratively
$$\sqrt{(X_i - X_j)'(S^{-1/2})'WS^{-1/2}(X_i - X_j)}$$
- Based upon evolutionary algorithm
- It is very slow (especially if you choose a reasonable `pop.size`)
- Can also do it with `MatchIt` or `GenMatch`

\footnotesize
```{r GE-matching, echo=FALSE, message=FALSE}
library(Matching)
library(rgenoud)

gmatch <- GenMatch(d.new$has.woman,
           d.new[,-d.new$has.woman],
           pop.size = 1000,ties=FALSE,print.level=0)
matches <- gmatch$matches[, 2]
match.data <- subset(d.new, trt == 1)
match.data <- rbind(match.data, d.new[matches, ])
trt.factor <- rep(c("Treat","Control"),c(N,N))
means <- apply(match.data[,-1],2,function(x) tapply(x,trt.factor,mean))
sds <- apply(match.data[,-1],2,function(x) tapply(x,trt.factor,sd))
t.p <- apply(match.data[,-1],2,function(x) t.test(x[trt], x[!trt])$p.value)
ks.p <- apply(match.data[,-1],2,function(x) ks.test(x[trt], x[!trt])$p.value)
```

# Genetic Matching
\footnotesize
```{r, GE-result, echo=FALSE}
#View matches balance
round(t(rbind(means, t.p)), 3)[c(1:6),]
```
- And then you can calculate the effect of interest


# CEM

- CEM creates bins along each covariate dimension (either pre-specified
or automatic)
-  Units lying in the same strata are then matched together
- Curse of dimensionality means that with lots of covariates, weâ€™ll only rarely
have units in the same strata.

# CEM
\footnotesize
```{r CE-matching, echo=FALSE}
library(cem)
cem.match <- cem(treatment = "has.woman", 
              data = d.new, 
              drop = "liberalOutcome")
cem.match
```

# CEM
\footnotesize
```{r CE-matching1, echo=FALSE}
# Hopefully you are lucky and you have more units
# If not, just tweak CEM

cutpoints <- list( median.ideo=c(0.3,0.5,0.7), 
              median.age= c(60,65))
cem.tweak.match <- cem(treatment = "has.woman", 
                     data = d.new, 
                     drop = "liberalOutcome", cutpoints = cutpoints)
cem.tweak.match
```

# Asymptotics of Matching
- Matching creates extra uncertainty (why?)
- What is the real standard error of $\hat{\tau}$?
\pause
- Roadmap:
    - Abadie and Imbens (2006): asymptotic distribution for NN matching (with replacement)
    - Abadie and Imbens (2011): debiased matching estimator
    - Abadie and Imbens (2008): boostrap doesn't work for matching
    - Abadie and Imbens (2012): matching as a martingale (NN without replacement)
    - Abadie and Imbens (2016): asymptotic distribution for PS matching
    - Otsu and Rai (2017): wild bootstrap for NN matching
    - Bodory et al. (2018): wild bootstrap for PS matching

# Asymptotics of NN Matching
- Denote $E\left[Y_i(D_i) | X_i \right]$ as $\mu_{D_i}(X_i)$, then $Y_i = \mu_{D_i}(X_i) + \epsilon_i$
- Match with K nearest neighbors; replacement is allowed; covariates can be continuous
$$\hat{\tau}_M = \frac{1}{N}\sum_{i=1}^{N}(\widehat{Y}_i(1) - \widehat{Y}_i(0))$$
\pause
- The bias from NN matching can be decomposed into three parts:
$$\hat{\tau}_M - \tau = \overline{\tau(X)} - \tau + E_M + B_M$$
where
$$\overline{\tau(X)} = \frac{1}{N}\sum_{i=1}^{N}(\mu_1(X_i) - \mu_0(X_i))$$
and
$$E_M = \frac{1}{N}\sum_{i=1}^{N}(2D_i - 1)(1+\frac{K_M(i)}{M})\epsilon_i$$

# Asymptotics of NN Matching
- Abadie and Imbens (2006) show that both $\overline{\tau(X)}$ (difference in conditional expectations) and $E_M$ (sum of residuals) are asymptotically unbiased.
- However, 
$$B_M = \frac{1}{N}\sum_{i=1}^{N}(2D_i - 1)\left[\frac{1}{M}\sum_{m=1}^{M}(\mu_{1-D_i}(X_i) - \mu_{1-D_i}(X_{j_m(i)}))\right]$$
is not.
\pause
- The bias caused by "mismatch"; it declines very slowly.
- The speed depends on the number of continuous covariates.
- $B_M$ actually converges to an exponential distribution.
- We may estimate $B_M$ directly using the serial estimator proposed by Newey (1995).
- Take-away: do not use bootstrap for NN matching!


# The benefits of IPW
- Hirano et al. (2003): the variance of IPW estimators can reach the Cramer-Rao lower bound
- What if we use the real propensity score? 
\pause
- The variance will be larger! (Hahn, 1998)
- Empirical propensity scores take into account all the actual imbalances in the sample
\pause
- If we run regression with IPW: Doubly robust estimation (Robins et al., 1994)
\pause
- Can be extended to panel data (dynamic treatment regime)

# Caveats for IPW
- It behaves poorly at the "tail" of the support
\pause
- One solution is to drop data at the tail part
- Changes the quantity of interest
\pause
- Ma and Wang (2019): asymptotic distribution for both trimed/untrimed IPW
- They also provide a bias correction method based on resampling

\footnotesize
```{r IPW, echo=FALSE}
base.model <- lm(liberalOutcome ~ has.woman + median.ideo + median.age + repub.majority + has.minority + maj.experienced + 
                   liberal.lower.direction, d.new)

ipw.logit <- trt + (1 - trt)/(1 - pscore.logit)

ipw.logit.mod <- lm(liberalOutcome ~ has.woman+median.ideo + median.age +repub.majority+has.minority + 
                      maj.experienced+liberal.lower.direction, d.new, weights=ipw.logit)
```

# CBPS and PW
- What can you do when the treatment is continuous? 
\pause
- Imai and Ratkovic (2013); Fong, Hazlett and Imai (2018): Covariate Balancing Propensity Score
- Idea: find weights that are orthogonal to $X$, $D$, and their interaction 
$$\sum_{i}^{N} w_i (X^{*}_{i}, D_{i}^{*}, X^{*}_{i}*D_{i}^{*}) = 0 \text{,    } \sum_{i}^{N} w_i = N$$
\pause
- Arbour and Dimmery (2019): use bootstrap to extract the information contained in propensity score
$$w_i = \frac{P(C_i = 1 | D_i, X_i)}{P(C_i = 0 | D_i, X_i)}$$
The two probabilities are obtained via machine learning

# An application of CBPS
- Wang and Wong (2018) \pause
- TA's QP \pause
- Does Hong Kong's Umbrella Movement reduce people's support for the opposition? \pause

# An application of CBPS
\begin{center}
\includegraphics[width=\linewidth, height=.6\linewidth]{/Users/yewang/Dropbox/CurrentProjects/HKElection/Graph/antigov_share_lm.pdf}
\end{center}
Driven by protest exposure, or other features of the central city?

# An application of CBPS
\begin{center}
\includegraphics[width=.5\linewidth, height=1.5\linewidth]{/Users/yewang/Dropbox/CurrentProjects/HKElection/Graph/CBPS_unweighted.pdf}
\includegraphics[width=.5\linewidth, height=1.5\linewidth]{/Users/yewang/Dropbox/CurrentProjects/HKElection/Graph/CBPS_weighted.pdf}
\end{center}


